# Smart Gallery â€” CLIP Fine-Tuning

___
## About
*Smart Gallery â€” CLIP Fine-Tuning is the training and experimentation module of the Smart Gallery project. It is responsible for preparing datasets, fine-tuning CLIP-based models (including ruCLIP), and managing experiments with various configurations. The pipeline supports automatic caption generation using Qwen-2.5, data management via SQLite, and model training in Jupyter notebooks.*

Key features:
- Dataset preparation using image-text pairs stored in SQLite  
- Image captioning pipeline powered by Qwen-2.5  
- Training and evaluation of ruCLIP and OpenCLIP variants  
- Modular architecture for managing datasets and experiments  
- Supports local experimentation through notebooks and CLI tools  

___
## Project Structure

<details>
  <summary>ðŸ“‚ clip_fine_tuning/</summary>
  <ul>
    <li>ðŸ“„ <code>pyproject.toml</code> â€” Project metadata and build system configuration</li>
    <li>ðŸ“„ <code>requirements.txt</code> â€” Python dependencies for fine-tuning and experiments</li>
    <details>
      <summary>ðŸ“‚ dataset/</summary>
      <ul>
        <details>
          <summary>ðŸ“‚ src/</summary>
          <ul>
            <li>ðŸ“„ <code>database.py</code> â€” Interface for accessing and querying the SQLite dataset</li>
            <li>ðŸ“„ <code>models.py</code> â€” Pydantic/ORM models used for dataset structure</li>
            <li>ðŸ“„ <code>repository.py</code> â€” Logic for loading and managing image-text pairs</li>
            <li>ðŸ“„ <code>ruclip_dataset.py</code> â€” Dataset wrapper for training with ruCLIP</li>
          </ul>
        </details>
        <li>ðŸ“„ <code>1. qwen25_test.ipynb</code> â€” Notebook for verifying Qwen-2.5 API keys</li>
        <li>ðŸ“„ <code>2. clip993.ipynb</code> â€” Captioning images with Qwen-2.5 for ruCLIP dataset</li>
        <li>ðŸ“„ <code>clip.db</code> â€” SQLite database with image-text pairs</li>
        <li>ðŸ“„ <code>qwen_api_keys.json</code> â€” API keys for Qwen model access</li>
      </ul>
    </details>
    <details>
      <summary>ðŸ“‚ models/</summary>
      <ul>
        <details>
          <summary>ðŸ“‚ fine-tuning/</summary>
          <ul>
            <li>ðŸ“„ <code>1. ruclip_clip993.ipynb</code> â€” Notebook for training ruCLIP on custom dataset</li>
          </ul>
        </details>
        <li>ðŸ“„ <code>1. open_clip.ipynb</code> â€” Experiment with OpenCLIP model</li>
        <li>ðŸ“„ <code>2. ruclip.ipynb</code> â€” Loading and using ruCLIP</li>
        <li>ðŸ“„ <code>3. ruclip_tiny.ipynb</code> â€” Experiment with ruCLIP tiny version</li>
        <li>ðŸ“„ <code>4. ruclip_clip993.ipynb</code> â€” Loading and using ruClip finetuned on clip993</li>
        <li>ðŸ“„ <code>base_clip.py</code> â€” Abstract class for CLIP-like models</li>
      </ul>
    </details>
  </ul>
</details>

___
## Technologies Used
![SQLAlchemy](https://img.shields.io/badge/ORM-SQLAlchemy-000000?logo=sqlalchemy) ![SQLite](https://img.shields.io/badge/Database-SQLite-003B57?logo=sqlite) ![DashScope](https://img.shields.io/badge/QwenAPI-DashScope-0064FF) ![TQDM](https://img.shields.io/badge/Progress-TQDM-4CAF50) ![Requests](https://img.shields.io/badge/HTTP-Requests-20232A?logo=python) ![OpenCLIP](https://img.shields.io/badge/Model-OpenCLIP-FF8C00) ![ruCLIP](https://img.shields.io/badge/Model-ruCLIP-orange) ![Torch](https://img.shields.io/badge/Fine--tuning-PyTorch-EE4C2C?logo=pytorch) ![HuggingFace](https://img.shields.io/badge/Hub-HuggingFace-FF4C7B?logo=huggingface) ![LiveLossPlot](https://img.shields.io/badge/Monitoring-LiveLossPlot-44CC11)